{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape EG data from NE Reval website\n",
    "\n",
    "epq 3/26/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup \n",
    "from datetime import datetime, timedelta, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for extract\n",
    "\n",
    "- lowest NE Reval account number\n",
    "- highest NE Reval account number\n",
    "- length of pause between requests (so we don't flood the server)\n",
    "- fixed text that precedes the account number in the URL\n",
    "- fixed text that follows the account number in the URL\n",
    "- path to directory to store the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1                                                #lowest account number\n",
    "stop  = 5600                                             #highest account number\n",
    "pause_seconds = 5                                        #wait between http requests (don't overwhelm the server)\n",
    "                                                         #fixed part of URL before account number\n",
    "url_part1 = 'https://data.nereval.com/PropertyDetail.aspx?town=East%20Greenwich&accountnumber='   \n",
    "url_part2 = '&card=1'                                    #fixed part of URL after account number\n",
    "path = 'data/eg03252021/'                                #path to output directory (directory must exist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pages for accounts in the range specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(start,stop+1):                        #range of account numbers to look for\n",
    "    url = url_part1 + str(i) + url_part2                 #construct URL for this account\n",
    "    r=requests.get(url)                                  #load the page                     \n",
    "    time.sleep(pause_seconds)                            #wait pause_seconds \n",
    "    if (r.status_code == 200):                           #status code is 200 if page was retrieved\n",
    "        t = r.content                                    #content is a python byte array\n",
    "        if (len(t) > 0):                                 #make sure there is something to process\n",
    "            fn = path + 'acct' + str(i) + '.pkl'         #build file name  acctxxxx.pkl\n",
    "            with open(fn,'wb') as binary_file:           #open the file write binary\n",
    "                pickle.dump(t, binary_file,fix_imports=False)   #dump the byte array to a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of the filenames for accounts that were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5165"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname, dirnames, filenames in os.walk(path):\n",
    "    # print path to all subdirectories first.\n",
    "    for subdirname in dirnames:\n",
    "        print(os.path.join(dirname, subdirname))\n",
    "len(filenames)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the html information with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accounts:  5165\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary\n",
    "\n",
    "nerd = {}\n",
    "\n",
    "# Read html files\n",
    "for filename in filenames:\n",
    "    if filename not in nerd.keys():\n",
    "        nerd[filename] = {}\n",
    "    fp=os.path.join(dirname, filename)\n",
    "    \n",
    "    town=fp.split('/')[1]\n",
    "    \n",
    "    with open(path + filename, 'rb') as handle:\n",
    "         txt = pickle.load(handle)\n",
    "        \n",
    "    soup = BeautifulSoup(txt,\"lxml\")\n",
    "    #soup = BeautifulSoup(txt,\"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        tbl          = soup.find('table', id=\"ParcelID_ParcelID\").findAll('td')         #get the ParcelID table entries\n",
    "        \n",
    "        nerd[filename]['ParcelID_ParcelID'] = {}\n",
    "        nerd[filename]['ParcelID_ParcelID']['ParcelID']     = tbl[1].font.text.strip()\n",
    "        nerd[filename]['ParcelID_ParcelID']['account']      = tbl[3].font.text.strip()\n",
    "        nerd[filename]['ParcelID_ParcelID']['State']        = tbl[5].font.text.strip()\n",
    "        nerd[filename]['ParcelID_ParcelID']['Card']         = tbl[7].font.text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        nerd[filename]['ParcelID_ParcelID'] = {}\n",
    "        \n",
    "    try:\n",
    "        tbl          = soup.find('table', id=\"Assessment_Assessment\").findAll('td')     #get the Assessment table entries\n",
    "\n",
    "        nerd[filename][\"Assessment_Assessment\"] = {}\n",
    "        nerd[filename][\"Assessment_Assessment\"]['Land'] = tbl[1].font.text.lstrip('$').replace(',','').strip()\n",
    "        nerd[filename][\"Assessment_Assessment\"]['Building'] = tbl[3].font.text.lstrip('$').replace(',','').strip()\n",
    "        nerd[filename][\"Assessment_Assessment\"]['Card_total'] = tbl[5].font.text.lstrip('$').replace(',','').strip()\n",
    "        nerd[filename][\"Assessment_Assessment\"]['Parcel_total'] = tbl[7].font.text.lstrip('$').replace(',','').strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        nerd[filename][\"Assessment_Assessment\"] = {}\n",
    "            \n",
    "    try:\n",
    "        tbl          = soup.find('table', id=\"LocationOwner_Location\").findAll('tr')     #get the location/owner table entries\n",
    "        \n",
    "        nerd[filename][\"LocationOwner_Location\"] = {}\n",
    "        nerd[filename][\"LocationOwner_Location\"]['Location'] = (tbl[0]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "        nerd[filename][\"LocationOwner_Location\"]['Owner'] = (tbl[1]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "        nerd[filename][\"LocationOwner_Location\"]['Owner2'] = (tbl[2]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "        nerd[filename][\"LocationOwner_Location\"]['Owner3'] = (tbl[3]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "        nerd[filename][\"LocationOwner_Location\"]['Address'] = (tbl[4]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "        nerd[filename][\"LocationOwner_Location\"]['Address2'] = (tbl[5]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "        nerd[filename][\"LocationOwner_Location\"]['Address3'] = (tbl[6]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        nerd[filename][\"LocationOwner_Location\"] = {}\n",
    "        \n",
    "    try:\n",
    "        tbl             = soup.find('table', id=\"BuildingInformation_Building\").findAll('tr')  #building info table entries\n",
    "        \n",
    "        nerd[filename][\"BuildingInformation_Building\"] = {}\n",
    "        try:\n",
    "            \n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Design'] = (tbl[0]).findAll('td')[1].font.text.strip()\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Year_built'] = (tbl[1]).findAll('td')[1].font.text.strip()\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Heat'] = (tbl[2]).findAll('td')[1].font.text.strip()\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Fireplaces'] = (tbl[3]).findAll('td')[1].font.text.strip()\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Rooms'] = (tbl[4]).findAll('td')[1].font.text.strip()\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Bedrooms'] = (tbl[5]).findAll('td')[1].font.text.strip()\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Bathrooms'] = (tbl[6]).findAll('td')[1].font.text.strip()\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Above_Ground_SF'] = (tbl[7]).findAll('td')[1].font.text.replace(',','').strip()\n",
    "        except IndexError:\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Design']          = ''\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Year_built']      = ''\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Heat']            = ''\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Fireplaces']      = ''\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Rooms']           = ''\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Bedrooms']        = ''\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Bathrooms']       = ''\n",
    "            nerd[filename][\"BuildingInformation_Building\"]['Above_Ground_SF'] = ''\n",
    "            \n",
    "    except AttributeError:\n",
    "        nerd[filename][\"BuildingInformation_Building\"] = {}\n",
    "        \n",
    "    try:\n",
    "        tbl          = soup.find('table', id=\"LandInformation_Land\") .findAll('td')     #get the Land info table entries\n",
    "        nerd[filename][\"LandInformation_Land\"] = {}\n",
    "        if (len(tbl) > 7):\n",
    "            nerd[filename][\"LandInformation_Land\"]['Land_area']    = tbl[1].font.text\n",
    "            nerd[filename][\"LandInformation_Land\"]['Zoning']       = tbl[3].font.text\n",
    "            nerd[filename][\"LandInformation_Land\"]['View']         = tbl[5].font.text\n",
    "            nerd[filename][\"LandInformation_Land\"]['Neighborhood'] = tbl[7].font.text\n",
    "            \n",
    "    except AttributeError:\n",
    "        nerd[filename][\"LandInformation_Land\"] = {}\n",
    "        \n",
    "    try:\n",
    "        tbl          = soup.find('table', id=\"PriorInformation_GridView2\").findAll('tr') #get prior info table entries\n",
    "\n",
    "        nerd[filename][\"PriorInformation_GridView2\"] = {}\n",
    "        \n",
    "        for n in range(1,len(tbl)):\n",
    "            nerd[filename][\"PriorInformation_GridView2\"][n] = {}\n",
    "            nerd[filename][\"PriorInformation_GridView2\"][n]['FY'] = (tbl[n]).findAll('td')[0].font.text\n",
    "            nerd[filename][\"PriorInformation_GridView2\"][n]['Land_value'] = (tbl[n]).findAll('td')[1].font.text.lstrip('$').replace(',','')\n",
    "            nerd[filename][\"PriorInformation_GridView2\"][n]['Building_value'] = (tbl[n]).findAll('td')[2].font.text.lstrip('$').replace(',','')\n",
    "            nerd[filename][\"PriorInformation_GridView2\"][n]['Outbuilding_value']  = (tbl[n]).findAll('td')[3].font.text.lstrip('$').replace(',','')\n",
    "            nerd[filename][\"PriorInformation_GridView2\"][n]['Total_value'] = (tbl[n]).findAll('td')[4].font.text.lstrip('$').replace(',','')\n",
    "\n",
    "    except AttributeError:\n",
    "        nerd[filename][\"PriorInformation_GridView2\"] = {}\n",
    "        \n",
    "    try:        \n",
    "        tbl          = soup.find('table', id=\"SaleInformation_Sales\").findAll('tr') #get prior info table entries\n",
    "\n",
    "        nerd[filename][\"SaleInformation_Sales\"] = {}\n",
    "        \n",
    "        for n in range(1,len(tbl)):\n",
    "            nerd[filename][\"SaleInformation_Sales\"][n] = {}\n",
    "            nerd[filename][\"SaleInformation_Sales\"][n]['Sale_date'] = (tbl[n]).findAll('td')[0].font.text\n",
    "            nerd[filename][\"SaleInformation_Sales\"][n]['Sale_price'] = (tbl[n]).findAll('td')[1].font.text.lstrip('$').replace(',','')\n",
    "            nerd[filename][\"SaleInformation_Sales\"][n]['Legal_reference'] = (tbl[n]).findAll('td')[2].font.text\n",
    "            nerd[filename][\"SaleInformation_Sales\"][n]['Instrument']  = (tbl[n]).findAll('td')[3].font.text\n",
    "        \n",
    "    except AttributeError:\n",
    "        nerd[filename][\"SaleInformation_Sales\"] = {}\n",
    "        \n",
    "    try:        \n",
    "        tbl          = soup.find('table', id=\"SubArea_SubArea\").findAll('td')      #subarea\n",
    "        \n",
    "        nerd[filename]['SubArea_SubArea'] = {}    #subarea\n",
    "        \n",
    "        if (len(tbl) > 1):\n",
    "            for i in np.arange(0,len(tbl),2):\n",
    "                nerd[filename]['SubArea_SubArea'][tbl[i].font.text.strip()] = tbl[i+1].font.text.strip()\n",
    "                \n",
    "    except AttributeError:\n",
    "        nerd[filename][\"SubArea_SubArea\"] = {}\n",
    "                \n",
    "\n",
    "    try:\n",
    "        tbl          = soup.find('table', id=\"YardItems_GridView1\").findAll('td')\n",
    "        \n",
    "        nerd[filename]['YardItems_GridView1'] = {}    #YardItems\n",
    "        if (len(tbl) > 1):\n",
    "            for i in np.arange(0,len(tbl),2):\n",
    "                nerd[filename]['YardItems_GridView1'][tbl[i].font.text.strip()] = tbl[i+1].font.text.strip()\n",
    "                \n",
    "    except AttributeError:\n",
    "        nerd[filename]['YardItems_GridView1'] = {}    #YardItems\n",
    "        \n",
    "print('Number of accounts: ',len(nerd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionary to a pickle file\n",
    "\n",
    "Filename is:     \n",
    "\n",
    "**NE_Reval_dictionary_MM_DD_YYYY.pkl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with  5165  accounts saved to file:  ../NE_Reval_dictionary_3_26_2021.pkl\n"
     ]
    }
   ],
   "source": [
    "current_date = date.today()\n",
    "fname = '../NE_Reval_dictionary_' + str(current_date.month) + '_' + \\\n",
    "    str(current_date.day) + '_' + str(current_date.year) + '.pkl'\n",
    "with open(fname, 'wb') as handle:\n",
    "    pickle.dump(nerd, handle)\n",
    "    \n",
    "print('Dictionary with ',len(nerd),' accounts saved to file: ',fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
