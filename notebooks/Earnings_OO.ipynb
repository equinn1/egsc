{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read earnings reports (OO version)\n",
    "\n",
    "E. Quinn 12/22/2019\n",
    "\n",
    "This notebook uses pdfminer to extract the information from the individual earnings report\n",
    "\n",
    "The documentation for pdfminer is at:\n",
    "\n",
    "https://buildmedia.readthedocs.org/media/pdf/pdfminer-docs/latest/pdfminer-docs.pdf\n",
    "\n",
    "Maintenance:\n",
    "\n",
    "* 3/6/2020  \n",
    "  * Add check date and number\n",
    "* 3/7/2020  \n",
    "  * Align personnel classes with support professionals structure\n",
    "  * Implement salary step capture for support professionals\n",
    "* 4/8/2020\n",
    "  * Rewrite logic to base data structure on check number and check date\n",
    "  * Simplify payment decoding logic to take advantage of having check date\n",
    "  * Data corrections for check dates and numbers:\n",
    "    * Adjust 5 check dates to aliign with nearest payday\n",
    "    * Generate 4 artificial check numbers for zero earnings lines\n",
    "* 4/18/2020\n",
    "  * Add FY2016, FY2015 and FY2019+FY2020ytd (!)\n",
    "  * Add code to move 12/25/2015 payroll to 12/24/2015 \n",
    "    \n",
    "* 7/8/2020\n",
    "  * Write data to pickle files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import standard python datascience packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cloudpickle\n",
    "import statistics as st\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "from datascience import *\n",
    "from scipy import stats\n",
    "from statistics import mode, StatisticsError\n",
    "from collections import Counter\n",
    "import uuid\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pdfminer packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LTTextBoxHorizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the directory we are running in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RIDE UCOA labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data (deserialize)\n",
    "with open('../../egsc/UCOA_labels.pkl', 'rb') as handle:\n",
    "    UCOA_labels = cloudpickle.load(handle)\n",
    "    \n",
    "help(UCOA_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EG accounting codes class\n",
    "\n",
    "provides descriptions for EG accounting codes and mapping to UCOA codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data (deserialize)\n",
    "with open('../../egsc/EG_acct_codes.pkl', 'rb') as handle:\n",
    "    EG_acct_codes = cloudpickle.load(handle)\n",
    "    \n",
    "help(EG_acct_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pay_check class\n",
    "\n",
    "Represents a line in the earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pay_check():                                                            #generic check class\n",
    "    def __init__(self,check_number,name,check_date,payperiod_object):         #constructor\n",
    "        self.check_number   = check_number\n",
    "        self.name           = name\n",
    "        self.check_date     = check_date\n",
    "        self.items          = {}\n",
    "        self.pay_period     = payperiod_object\n",
    "        self.fiscal_year    = self.pay_period.get_fiscal_year(check_date)\n",
    "        self.school_year    = self.pay_period.get_school_year(check_date)\n",
    "        self.calendar_year  = check_date.year\n",
    "        return\n",
    "\n",
    "    def get_name(self):\n",
    "        return(self.name)\n",
    "        \n",
    "    def get_date(self):\n",
    "        return(self.check_date)\n",
    "    \n",
    "    def get_number(self):\n",
    "        return(self.check_number)\n",
    "    \n",
    "    def get_fiscal_year(self):\n",
    "        return(self.fiscal_year)\n",
    "\n",
    "    def get_school_year(self):\n",
    "        return(self.school_year)\n",
    "\n",
    "    def get_calendar_year(self):\n",
    "        return(self.calendar_year)\n",
    "    \n",
    "    def get_items(self):\n",
    "        return(self.items)\n",
    "    \n",
    "    def add_item(self,fund,acct,obj,position,rate,earnings,acct_desc,obj_desc,acct_UCOA,stepinfo):\n",
    "        item_number = len(self.items) + 1\n",
    "        self.items[item_number] = {'fund':fund,'acct':acct,'obj':obj,'position':position, \\\n",
    "            'rate':rate,'earnings':earnings,'acct_desc':acct_desc,'obj_desc':obj_desc, \\\n",
    "            'acct_UCOA':acct_UCOA,'step_info':stepinfo}\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payperiod class\n",
    "\n",
    "Represents a two-week pay period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../payperiod.pkl', 'rb') as handle:\n",
    "    payperiod = cloudpickle.load(handle)\n",
    "    \n",
    "help(payperiod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher salary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../teacher_salary_matrix.pkl', 'rb') as handle:\n",
    "    teacher_salary_matrix = cloudpickle.load(handle)\n",
    "    \n",
    "help(teacher_salary_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### personnel classes\n",
    "\n",
    "provides functionality related to HR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person():                                                         #generic employee class\n",
    "    def __init__(self,name,ppo,ega,ula):                                #constructor\n",
    "        self.name = name\n",
    "        self.payperiods = {}                                            #payperiods\n",
    "        self.ppo = ppo                                                  #payperiod class object\n",
    "        self.ega = ega                                                  #EG accounting class\n",
    "        self.ula = ula                                                  #RIDE UCOA labels object\n",
    "        self.retirement = np.NaN                                        #retirement date\n",
    "        return\n",
    "        \n",
    "    def add_check(self,check_date,check):\n",
    "        if check_date not in self.payperiods.keys():\n",
    "            self.payperiods[check_date] = {}\n",
    "        check_seq = 1+len(self.payperiods[check_date])\n",
    "        self.payperiods[check_date][check_seq] = check\n",
    "        return\n",
    "    \n",
    "    def get_name(self):                                                 #return name of person\n",
    "        return(self.name)\n",
    "    \n",
    "    def get_position(self,syear,check_no):                              #return name of person\n",
    "        return(self.name)\n",
    "    \n",
    "    def get_payperiods(self):\n",
    "        return(self.payperiods)\n",
    "    \n",
    "    def set_retirement(self,retdate):\n",
    "        self.retirement = retdate\n",
    "        return\n",
    "    \n",
    "    def get_retirement(self):\n",
    "        return(self.retirement)\n",
    "    \n",
    "    def get_payperiod(self,check_date):\n",
    "        try:\n",
    "            return(self.payperiods[check_date])\n",
    "        except IndexError:\n",
    "            return({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the pdf and create a dictionary with the contents of each text box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function read_pdf() reads a PDF and returns a dictionary containing the contents\n",
    "\n",
    "Strategy for this document:  \n",
    "\n",
    "Save information from each element in the LTTextBox objects in a dictionary including:\n",
    "\n",
    "- x0 horizontal coordinate of the upper left corner of the text box\n",
    "- x1 horizontal coordinate of the lower right corner of the text box\n",
    "- y0 vertical coordinate of the upper left corner of the text box\n",
    "- y1 vertical coordinate of the lower right corner of the text box\n",
    "- page number \n",
    "- sequence number of text box within this page\n",
    "- text contained in the text box, converted to ascii\n",
    "\n",
    "Parsing the text is complicated by the fact that that a text box may span multiple columns and/or rows, and the text box groupings vary quite a bit depending on the page contents and layout.\n",
    "\n",
    "However, with a bit of luck the structure of the document will allow the contents to be deciphered with the following heuristics:\n",
    "\n",
    "- Text boxes containing left justified columns will tend to have nearly the same x0 coordinates\n",
    "- Text boxes containing right justified columns will tend to have nearly the same x1 coordinates\n",
    "- The codes for fund, account code, and object code are numeric and have fixed lengths\n",
    "- Extraneous information is often preceded or followed by a series of underscore and newline characters\n",
    "- Last name can be distinguished because is the only field that is all characters followed by a comma\n",
    "- Last name may be preceded by between one and three numerical fields:  fund, account, object.  If it is, the x0 value is shifted to the left.\n",
    "    - Three numerical fields precede the name:  assume they are fund, account, object\n",
    "    - Two numerical fields precede the name: assume they are account, object\n",
    "    - One numerical field precedes the name: assume it is object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    document = open(path, 'rb')                                     #read a pdf and create a document object\n",
    "    rsrcmgr = PDFResourceManager()                                  #create a resource manager\n",
    "    laparams = LAParams()                                           #set the parameters for analysis\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)          #create a PDF page aggregator object\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    pdf={}                                                          #dictionary to hold the results\n",
    "\n",
    "    pageno = -1                                                     #initialize page coounter to zero\n",
    "\n",
    "    for page in PDFPage.get_pages(document):                        #loop through the pdf page by page\n",
    "        pageno = pageno + 1                                         #increment the page number\n",
    "        pdf[pageno] = {}                                            #dictionary for this page\n",
    "        interpreter.process_page(page)                              # receive the LTPage object for the page.\n",
    "        layout = device.get_result()                                # create layout object\n",
    "        tbox_no=0                                                   # index for element number\n",
    "        for element in layout:\n",
    "            if (type(element).__name__=='LTTextBoxHorizontal'):     #loop through text boxes\n",
    "                tbox_no += 1                                        #increment text box number\n",
    "                pdf[pageno][tbox_no] = {}                           #dictionary for text boxes within page\n",
    "                x0 = round(element.x0,2)                            #x0 coordinate of textbox corner\n",
    "                x1 = round(element.x1,2)                            #x1 coordinate of textbox corner\n",
    "                y0 = round(element.y0,2)                            #y0 coordinate of textbox corner\n",
    "                y1 = round(element.y1,2)                            #y1 coordinate of textbox corner\n",
    "                txt = element.get_text().encode('ascii', 'ignore')  #text converted to ascii\n",
    "                pdf[pageno][tbox_no]['x0'] = x0                     #create x0 coordinate entry\n",
    "                pdf[pageno][tbox_no]['x1'] = x1                     #create x1 coordinate entry\n",
    "                pdf[pageno][tbox_no]['y0'] = y0                     #create y0 coordinate entry\n",
    "                pdf[pageno][tbox_no]['y1'] = y1                     #create y1 coordinate entry\n",
    "\n",
    "                pdf[pageno][tbox_no]['text'] = ''.join(chr(c) for c in txt) #convert bytes to string\n",
    "    return(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the commas from earnings and rate values\n",
    "\n",
    "def remove_commas(st):\n",
    "    newstr = st.replace(',','')                     #remove commas from string\n",
    "    return(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the headings fields \n",
    "\n",
    "def remove_headings(st):\n",
    "    lines = st.split('\\n')                         #split the string at newline characters '\\n'\n",
    "    for line in lines:                             #loop through the resulting lines\n",
    "        if (line.startswith('FUND ') |\\\n",
    "           (line.startswith('POSITION')) |\\\n",
    "           (line.startswith('RATE')) |\\\n",
    "           (line.startswith('ACCT-')) |\\\n",
    "           (line.startswith('CHECK')) |\\\n",
    "           (line.startswith('_'))):                #check for strings that appear only in headings\n",
    "            try:\n",
    "                newline_index = st.index('\\n')     #if present, remove this line from the text string\n",
    "                st = st[newline_index+1:]\n",
    "            except ValueError:\n",
    "                print('Value Error',st)            #recover from Value Error and print string\n",
    "        else:\n",
    "            return(st)                             #if no headings, just return\n",
    "    return('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2015 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p15 = read_pdf('../../finance_subcommittee/earnings/Munis_7-1-2014_to_6-30-2015.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2016 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p16 = read_pdf('../../finance_subcommittee/earnings/Munis_7-1-2015_to_6-30-2016.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2017 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p17 = read_pdf('../FY17 Gene_Redacted.pdf')\n",
    "p17 = read_pdf('../../finance_subcommittee/earnings/Munis_7-1-2016_to_6-30-2017.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2018 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p18 = read_pdf('../FY18 Gene_Redacted.pdf')\n",
    "p18 = read_pdf('../../finance_subcommittee/earnings/Munis_7-1-2017_to_6-30-2018.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2019 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p19 = read_pdf('../../finance_subcommittee/earnings/Munis_7-1-2018_to_7-1-2019.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2020 through current report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p20 = read_pdf('../../finance_subcommittee/earnings/Munis_7-1-2019_to_current.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary with only those text boxes containing names\n",
    "\n",
    "Use the following algorithm to identify text boxes that contain names:\n",
    "\n",
    "- x0, horizontal coordinate of the upper left corner of the text box, is less than 162\n",
    "- the text string contains at least one comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(dct):\n",
    "\n",
    "    dnames = {}\n",
    "\n",
    "    fund = ''\n",
    "    acct = ''\n",
    "    obj  = ''\n",
    "    \n",
    "    for page in sorted(dct.keys()):                                #loop through text box dictionary by page # \n",
    "        if (page not in dnames.keys()):                            #page number is highest level key\n",
    "            dnames[page] = {}                                      #initialize entry for this page\n",
    "        for tb in sorted(dct[page].keys()):                        #loop through all text boxes on this page\n",
    "            if (dct[page][tb]['x0'] < 162.0):                      #those with names start to the left of x0=162\n",
    "                txt = str(dct[page][tb]['text'])                   #convert the 'text' element to a string\n",
    "                if (',' in txt):                                   #every name contains a comma\n",
    "                    txt = remove_headings(txt)\n",
    "                    lines = txt.split('\\n')                        #split text into lines\n",
    "                    words = lines[0].split()                       #split first line into words\n",
    "                    for word in words:                             #loop through and strip out fund, acct, obj\n",
    "                        if (word.isdigit()):\n",
    "                            if (len(word)==4):                     # 4 digits means fund\n",
    "                                fund = word\n",
    "                            if (len(word)==8):                     # 8 digits means acct-code\n",
    "                                acct = word\n",
    "                            if (len(word)==5):                     # 5 digits means obj\n",
    "                                obj = word\n",
    "                            txt = txt[len(word)+1:]                # remove fund/acct/obj from txt\n",
    "                    dnames[page][tb] = {}                          #initialize dictionary for this page\n",
    "                    dnames[page][tb]['x0'] = dct[page][tb]['x0']\n",
    "                    dnames[page][tb]['x1'] = dct[page][tb]['x1']\n",
    "                    dnames[page][tb]['y0'] = dct[page][tb]['y0']\n",
    "                    dnames[page][tb]['y1'] = dct[page][tb]['y1']\n",
    "                    dnames[page][tb]['fund'] = fund\n",
    "                    dnames[page][tb]['acct'] = acct\n",
    "                    dnames[page][tb]['obj'] = obj\n",
    "                    dnames[page][tb]['text'] = txt\n",
    "    return(dnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate text boxes that overlap on the vertical scale and contain names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_name_boxes(names):\n",
    "    newnames = {}\n",
    "    \n",
    "    for page in sorted(names.keys()):                                        #loop through pages of pdf\n",
    "        newnames[page] = {}                                                  #initialize new names dictionary\n",
    "        skip = make_array()                                                  #initialize list of boxes to skip\n",
    "    \n",
    "        for tb in sorted(names[page].keys()):                                #loop through text boxes on this page\n",
    "            for tb2 in sorted(names[page].keys()):                           #compare this one to the others\n",
    "                if ((tb2 > tb) & \\\n",
    "                    (names[page][tb]['y0'] <= names[page][tb2]['y1']) & \\\n",
    "                    (names[page][tb2]['y0'] <= names[page][tb]['y1'])):      \n",
    "                    d = {}                                                   #initialize replacement entry\n",
    "                    d['x0'] = names[page][tb]['x0']                          #keep x0    \n",
    "                    d['x1'] = names[page][tb2]['x1']                         #replace x1 with tb2 value\n",
    "                    d['y0'] = names[page][tb2]['y0']                         #replace y0 with tb2 value\n",
    "                    d['y1'] = names[page][tb]['y1']                          #keep y1 value\n",
    "                    d['text'] = names[page][tb]['text'] +\\\n",
    "                        names[page][tb2]['text']                             #contatenate text strings\n",
    "                    d['fund'] = names[page][tb]['fund']                      #copy fund, acct, and obj\n",
    "                    d['acct'] = names[page][tb]['acct']\n",
    "                    d['obj'] = names[page][tb]['obj']\n",
    "                    newnames[page][tb2] = d                                  #plug into dictionary\n",
    "                    skip = np.append(skip,tb)                                #add old boxes to skip list\n",
    "                    skip = np.append(skip,tb2)\n",
    "            if (tb not in skip):                                             #if no match, check skip list \n",
    "                newnames[page][tb] = names[page][tb]                         #just copy if not in skip list\n",
    "                    \n",
    "    return(newnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary with earnings report items by text box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combdd(cn,pdf):\n",
    "    \n",
    "    dd = {}\n",
    "    \n",
    "    for page in sorted(cn.keys()):\n",
    "        if page not in dd.keys():\n",
    "            dd[page] = {}\n",
    "        for tb in sorted(cn[page].keys()):                               #loop through consolidated name textboxes\n",
    "            dd[page][tb] = cn[page][tb]\n",
    "            y0  = dd[page][tb]['y0']                                      #extract vertical coordinates\n",
    "            y1  = dd[page][tb]['y1']\n",
    "            txt = dd[page][tb]['text']                           #extract text\n",
    "            for tb2 in sorted(pdf[page].keys()):                            #loop through the other boxes in pdf\n",
    "                if (tb != tb2):                                             #ignore if same box as names\n",
    "                    tx0 = pdf[page][tb2]['x0']                              #get horizontal offset\n",
    "                    ty0 = pdf[page][tb2]['y0']                              #check whether the vertical \n",
    "                    ty1 = pdf[page][tb2]['y1']                              #range of this box overlaps that\n",
    "                    if ((y0 <= ty1) & (ty0 <= y1)):                         #of the name box\n",
    "                        txt = remove_headings(pdf[page][tb2]['text'])\n",
    "                        if ((312.0 < tx0) & (tx0 < 316.0)):                 #match to DATE/NUMBER\n",
    "                            dd[page][tb]['numbers1'] = txt\n",
    "                        if ((383.0 < tx0) & (tx0 < 395.0)):                 #match to NUMBER\n",
    "                            if 'numbers2' not in dd[page][tb].keys():\n",
    "                                dd[page][tb]['numbers2'] = txt\n",
    "                            else:\n",
    "                                dd[page][tb]['numbers2'] += txt\n",
    "                        if ((437.0 < tx0) & (tx0 < 440.0)):                 #match to POSITION\n",
    "                            dd[page][tb]['positions'] = txt\n",
    "                        if ((509.0 < tx0) & (tx0 < 533.0)):                 #match to RATE \n",
    "                            dd[page][tb]['rates'] = remove_commas(txt)\n",
    "                        if ((558.0 < tx0) & (tx0 < 630.0)):                 #match to ACCT-EARNINGS\n",
    "                            dd[page][tb]['earnings'] = remove_commas(txt)\n",
    "\n",
    "    return(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble data elements across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(nn):\n",
    "    \n",
    "    lld = {}\n",
    "    \n",
    "    for page in sorted(nn.keys()):\n",
    "        if page not in lld.keys():\n",
    "            lld[page] = {}\n",
    "        for tb in sorted(nn[page].keys()):\n",
    "            if tb not in lld[page].keys():\n",
    "                lld[page][tb]              = {}\n",
    "                lld[page][tb]['names']     = []\n",
    "                lld[page][tb]['checks']    = []\n",
    "                lld[page][tb]['dates']     = []\n",
    "                lld[page][tb]['rates']     = []\n",
    "                lld[page][tb]['earnings']  = []\n",
    "                lld[page][tb]['positions'] = []\n",
    "                lld[page][tb]['fund']      = ''\n",
    "                lld[page][tb]['acct']      = ''\n",
    "                lld[page][tb]['obj']       = ''\n",
    "            txt = nn[page][tb]['text']\n",
    "            words = txt.split('\\n')\n",
    "            for word in words:\n",
    "                if (len(word) > 1):\n",
    "                    lld[page][tb]['names'].append(word)\n",
    "            if 'numbers1' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['numbers1']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if word.isdigit():\n",
    "                        lld[page][tb]['checks'].append(word)\n",
    "                    elif '/' in word:\n",
    "                        lld[page][tb]['dates'].append(word)\n",
    "            if 'numbers2' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['numbers2']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if word.isdigit():\n",
    "                        lld[page][tb]['checks'].append(word)\n",
    "            if 'rates' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['rates']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if '.' in word:\n",
    "                        lld[page][tb]['rates'].append(float(word))\n",
    "            if 'positions' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['positions']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if len(word)>1:\n",
    "                        lld[page][tb]['positions'].append(word)\n",
    "            if 'fund' in nn[page][tb].keys():\n",
    "                lld[page][tb]['fund'] = nn[page][tb]['fund']\n",
    "            if 'acct' in nn[page][tb].keys():\n",
    "                lld[page][tb]['acct'] = nn[page][tb]['acct']\n",
    "            if 'obj' in nn[page][tb].keys():\n",
    "                lld[page][tb]['obj'] = nn[page][tb]['obj']\n",
    "            if 'earnings' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['earnings']\n",
    "                had_underscore = False\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if '.' in word:\n",
    "                        if not had_underscore: \n",
    "                            lld[page][tb]['earnings'].append(float(word))\n",
    "                            had_underscore = False\n",
    "                    elif '_' in word:\n",
    "                        had_underscore = True\n",
    "            if (len(lld[page][tb]['checks']) < len(lld[page][tb]['dates'])):\n",
    "                new_checks = []\n",
    "                check_index = 0\n",
    "                for i in np.arange(len(lld[page][tb]['earnings'])):\n",
    "                    if (lld[page][tb]['earnings'][i] > 0.0):\n",
    "                        new_checks.append(lld[page][tb]['checks'][check_index])\n",
    "                        check_index += 1\n",
    "                    else:\n",
    "                        new_checks.append('gen'+str(page) + '-' + str(tb) + '-' + str(i))\n",
    "                        print(\"inserting check number: \",page,tb,i)\n",
    "                lld[page][tb]['checks'] = new_checks\n",
    "    return(lld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the earnings report extracts and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_earnings(pdf):\n",
    "    nnd = get_names(pdf)\n",
    "    cnd = consolidate_name_boxes(nnd)\n",
    "    newnames = combdd(cnd,pdf)\n",
    "    lld = get_lines(newnames)\n",
    "    return(lld)\n",
    "\n",
    "ll={}\n",
    "\n",
    "ll[2015] = process_earnings(p15)\n",
    "ll[2016] = process_earnings(p16)\n",
    "ll[2017] = process_earnings(p17)\n",
    "ll[2018] = process_earnings(p18)\n",
    "ll[2019] = process_earnings(p19)\n",
    "ll[2020] = process_earnings(p20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check earnings against totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totearn = {}\n",
    "\n",
    "gtot = 0.0\n",
    "\n",
    "for year in ll.keys():\n",
    "    if year not in totearn.keys():\n",
    "        totearn[year] = 0.0\n",
    "    for page in ll[year].keys():\n",
    "        for tb in ll[year][page].keys():\n",
    "            for amt in ll[year][page][tb]['earnings']:\n",
    "                totearn[year] += amt\n",
    "    gtot += totearn[year]\n",
    "\n",
    "print(round(totearn[2015],2))       #FY2015 earnings report total is $20,527,796.79\n",
    "print(round(totearn[2016],2))       #FY2016 earnings report total is $21,988,876.13\n",
    "print(round(totearn[2017],2))       #FY2017 earnings report total is $22,608,024.34\n",
    "print(round(totearn[2018],2))       #FY2018 earnings report total is $22,409,915.41\n",
    "print(round(totearn[2019],2))       #FY2019 earnings report total is $23,372,079.04\n",
    "print(round(totearn[2020],2))       #FY2020ytd earnings report total is $19,489,886.57\n",
    "print(gtot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary of checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checks = {}\n",
    "\n",
    "paydays = payperiod.get_payperiods()\n",
    "\n",
    "for year in ll.keys():\n",
    "    for page in ll[year].keys():\n",
    "        for tb in ll[year][page].keys():\n",
    "            check_numbers = ll[year][page][tb]['checks']\n",
    "            names         = ll[year][page][tb]['names']\n",
    "            check_dates   = ll[year][page][tb]['dates']\n",
    "            fund          = ll[year][page][tb]['fund']\n",
    "            acct          = ll[year][page][tb]['acct']\n",
    "            obj           = ll[year][page][tb]['obj']\n",
    "            positions     = ll[year][page][tb]['positions']\n",
    "            rates         = ll[year][page][tb]['rates']\n",
    "            earnings      = ll[year][page][tb]['earnings']\n",
    "            obj_desc      = UCOA_labels.get_label('Obj',obj)\n",
    "            acct_desc     = EG_acct_codes.get_eg_acct_desc(acct)\n",
    "            acct_UCOA     = EG_acct_codes.get_eg_acct_UCOA(acct)\n",
    "            \n",
    "            for i in np.arange(len(check_numbers)):\n",
    "                \n",
    "                check_number    = check_numbers[i]\n",
    "                name            = names[i]\n",
    "                date_str        = check_dates[i]\n",
    "                position        = positions[i]\n",
    "                rate            = rates[i]\n",
    "                earned          = earnings[i]\n",
    "                \n",
    "                words = date_str.split('/')\n",
    "                check_date   = date(int(words[2]),int(words[0]),int(words[1]))\n",
    "                if (check_date not in paydays.keys()):\n",
    "                    new_date = payperiod.get_previous_payday(check_date)\n",
    "                    print('adjusting date',name,check_date,new_date)\n",
    "                    check_date = new_date\n",
    "                stepdata = {}\n",
    "                if (position == 'TEACHER'):\n",
    "                    stepdata = teacher_salary_matrix.decode_earnings(check_date,rate,earned,payperiod)\n",
    "                    if (len(stepdata['step']) < 1):\n",
    "                        stepdata={}\n",
    "                if check_number not in checks.keys():\n",
    "                    checks[check_number] = pay_check(check_number,name,check_date,payperiod)\n",
    "\n",
    "                checks[check_number].add_item(fund,acct,obj,position,rate,earned, \\\n",
    "                            acct_desc,obj_desc,acct_UCOA,stepdata)\n",
    "                \n",
    "print('Number of checks: ',len(checks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../finance_subcommittee/earnings_checks.pkl', 'wb') as handle:\n",
    "    cloudpickle.dump(checks, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {}\n",
    "\n",
    "people_uuid = {}\n",
    "\n",
    "for ckno in checks.keys():\n",
    "    ck   = checks[ckno]\n",
    "    name = ck.get_name()\n",
    "    ckdate = ck.get_date()\n",
    "    if name not in people.keys():\n",
    "        people[name] = Person(name,payperiod,EG_acct_codes,UCOA_labels)\n",
    "        people_uuid[name] = uuid.uuid4()\n",
    "\n",
    "    people[name].add_check(ckdate,ck)\n",
    "        \n",
    "print(len(people))\n",
    "\n",
    "with open('../../finance_subcommittee/earnings_uuid.pkl', 'wb') as handle:\n",
    "    cloudpickle.dump(people_uuid, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set retirement dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['ELSON, SUSAN E'].set_retirement(date(2017,5,26))\n",
    "people['KOWAL, MAUREEN E'].set_retirement(date(2020,5,22))\n",
    "people['CAVANAUGH, JUDITH L'].set_retirement(date(2020,5,22))\n",
    "people['HOFFMANN, ILENE B'].set_retirement(date(2020,5,22))\n",
    "people['LYONS, EILEEN C'].set_retirement(date(2020,5,22))\n",
    "people['HADFIELD, RENEE M'].set_retirement(date(2020,5,22))\n",
    "people['JOHNSON, TRESSA'].set_retirement(date(2020,5,22))\n",
    "people['MCCOWAN, BARBARA C'].set_retirement(date(2020,5,22))\n",
    "people['HOSTETLER, LYN A'].set_retirement(date(2020,5,22))\n",
    "people['MACARUSO, JOANNE E'].set_retirement(date(2020,5,22))\n",
    "people['MALLOZZI, JOANN S'].set_retirement(date(2020,5,22))\n",
    "people['ISIBEL, DAVID R'].set_retirement(date(2020,5,22))\n",
    "people['DEPASQUALE, HELEN L'].set_retirement(date(2020,5,22))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../finance_subcommittee/earnings_people.pkl', 'wb') as handle:\n",
    "    cloudpickle.dump(people, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    try:\n",
    "        return occurence_count.most_common(1)[0][0]\n",
    "    except IndexError:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dictionary of earnings history summaries by person with totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehist = {}\n",
    "\n",
    "for name in people.keys():\n",
    "    if name not in ehist.keys():\n",
    "        ehist[name] = {}\n",
    "    ppds = people[name].get_payperiods()\n",
    "    for ckdate in ppds.keys():\n",
    "        if ckdate not in ehist[name].keys():\n",
    "            ehist[name][ckdate] = {}\n",
    "        for seq in ppds[ckdate].keys():\n",
    "            check = ppds[ckdate][seq]\n",
    "            items = check.get_items()\n",
    "            for key in items.keys():\n",
    "                obj = items[key]['obj']\n",
    "                if obj not in ehist[name][ckdate].keys():\n",
    "                    ehist[name][ckdate][obj] = {}\n",
    "                    ehist[name][ckdate][obj]['earnings'] = []\n",
    "                    ehist[name][ckdate][obj]['rate'] = []\n",
    "                    ehist[name][ckdate][obj]['acct'] = []\n",
    "                    ehist[name][ckdate][obj]['acct6'] = []\n",
    "                    ehist[name][ckdate][obj]['step'] = []\n",
    "                    ehist[name][ckdate][obj]['payments'] = []\n",
    "                    ehist[name][ckdate][obj]['fte'] = []\n",
    "                    ehist[name][ckdate][obj]['mindiff'] = []\n",
    "                    ehist[name][ckdate][obj]['salary'] = []\n",
    "                    ehist[name][ckdate][obj]['position'] = []\n",
    "                ehist[name][ckdate][obj]['acct'].append(items[key]['acct'])\n",
    "                ehist[name][ckdate][obj]['acct6'].append(np.floor(int(items[key]['acct'])/100))\n",
    "                ehist[name][ckdate][obj]['rate'].append(items[key]['rate'])\n",
    "                ehist[name][ckdate][obj]['earnings'].append(items[key]['earnings'])\n",
    "                ehist[name][ckdate][obj]['position'].append(items[key]['position'])\n",
    "                if 'step' in items[key]['step_info'].keys():\n",
    "                    ehist[name][ckdate][obj]['step'].append(items[key]['step_info']['step'])\n",
    "                if 'mindiff' in items[key]['step_info'].keys():\n",
    "                    ehist[name][ckdate][obj]['mindiff'].append(items[key]['step_info']['mindiff'])\n",
    "                if 'payments' in items[key]['step_info'].keys():\n",
    "                    ehist[name][ckdate][obj]['payments'].append(items[key]['step_info']['payments'])\n",
    "                if 'salary' in items[key]['step_info'].keys():\n",
    "                    ehist[name][ckdate][obj]['salary'].append(items[key]['step_info']['salary'])\n",
    "                if 'fte' in items[key]['step_info'].keys():\n",
    "                    ehist[name][ckdate][obj]['fte'].append(items[key]['step_info']['fte'])\n",
    "                ehist[name][ckdate][obj]['total_earnings'] = \\\n",
    "                    np.sum(ehist[name][ckdate][obj]['earnings'])\n",
    "                ehist[name][ckdate][obj]['payments_mode']=\\\n",
    "                    most_frequent(ehist[name][ckdate][obj]['payments'])\n",
    "                ehist[name][ckdate][obj]['step_mode']=\\\n",
    "                    most_frequent(ehist[name][ckdate][obj]['step'])\n",
    "                ehist[name][ckdate][obj]['salary_mode']=\\\n",
    "                    most_frequent(ehist[name][ckdate][obj]['salary'])\n",
    "                ehist[name][ckdate][obj]['acct6_mode']=\\\n",
    "                    most_frequent(ehist[name][ckdate][obj]['acct6'])\n",
    "                acct_mode = most_frequent(ehist[name][ckdate][obj]['acct'])\n",
    "                ehist[name][ckdate][obj]['acct_mode']=acct_mode\n",
    "                ehist[name][ckdate][obj]['fte_mode']=\\\n",
    "                    most_frequent(ehist[name][ckdate][obj]['fte'])\n",
    "                ehist[name][ckdate][obj]['acct6_desc'] =\\\n",
    "                    EG_acct_codes.get_eg_acct_desc6(str(int(ehist[name][ckdate][obj]['acct6_mode'])))\n",
    "                ehist[name][ckdate][obj]['acct_desc'] =\\\n",
    "                    EG_acct_codes.get_eg_acct_desc(str(int(ehist[name][ckdate][obj]['acct_mode'])))\n",
    "                ehist[name][ckdate][obj]['ucoa'] = EG_acct_codes.get_UCOA_from_acct(acct_mode,UCOA_labels)\n",
    "                total_fte = \\\n",
    "                    ehist[name][ckdate][obj]['total_earnings']/(\\\n",
    "                    ehist[name][ckdate][obj]['salary_mode']/\\\n",
    "                    ehist[name][ckdate][obj]['payments_mode'])\n",
    "                if (len(ehist[name][ckdate][obj]['fte'])==1):\n",
    "                    total_fte = ehist[name][ckdate][obj]['fte'][0]\n",
    "                if (total_fte > 1.0):\n",
    "                    total_fte = 1.0\n",
    "                ehist[name][ckdate][obj]['total_fte'] = total_fte\n",
    "        \n",
    "len(ehist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../finance_subcommittee/earnings_ehist.pkl', 'wb') as handle:\n",
    "    cloudpickle.dump(ehist, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
