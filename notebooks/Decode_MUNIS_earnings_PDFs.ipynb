{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode_MUNIS_earnings_PDFs\n",
    "\n",
    "E. Quinn 12/22/2019\n",
    "\n",
    "This notebook uses pdfminer to extract the information from the individual earnings report\n",
    "\n",
    "The documentation for pdfminer is at:\n",
    "\n",
    "https://buildmedia.readthedocs.org/media/pdf/pdfminer-docs/latest/pdfminer-docs.pdf\n",
    "\n",
    "Maintenance:\n",
    "\n",
    "* 3/6/2020  \n",
    "  * Add check date and number\n",
    "* 3/7/2020  \n",
    "  * Align personnel classes with support professionals structure\n",
    "  * Implement salary step capture for support professionals\n",
    "* 4/8/2020\n",
    "  * Rewrite logic to base data structure on check number and check date\n",
    "  * Simplify payment decoding logic to take advantage of having check date\n",
    "  * Data corrections for check dates and numbers:\n",
    "    * Adjust 5 check dates to aliign with nearest payday\n",
    "    * Generate 4 artificial check numbers for zero earnings lines\n",
    "* 4/18/2020\n",
    "  * Add FY2016, FY2015 and FY2019+FY2020ytd (!)\n",
    "  * Add code to move 12/25/2015 payroll to 12/24/2015 \n",
    "    \n",
    "To do:\n",
    "* Replace computed salary matrix for FY2020 with numbers from contract (correct small rounding errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import standard python datascience packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "from datascience import *\n",
    "import uuid\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the directory we are running in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gquinn/EG/school_committee/finance_subcommittee/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pdfminer packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LTTextBoxHorizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the pdf and create a dictionary with the contents of each text box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function read_pdf() reads a PDF and returns a dictionary containing the contents\n",
    "\n",
    "Strategy for this document:  \n",
    "\n",
    "Save information from each element in the LTTextBox objects in a dictionary including:\n",
    "\n",
    "- x0 horizontal coordinate of the upper left corner of the text box\n",
    "- x1 horizontal coordinate of the lower right corner of the text box\n",
    "- y0 vertical coordinate of the upper left corner of the text box\n",
    "- y1 vertical coordinate of the lower right corner of the text box\n",
    "- page number \n",
    "- sequence number of text box within this page\n",
    "- text contained in the text box, converted to ascii\n",
    "\n",
    "Parsing the text is complicated by the fact that that a text box may span multiple columns and/or rows, and the text box groupings vary quite a bit depending on the page contents and layout.\n",
    "\n",
    "However, with a bit of luck the structure of the document will allow the contents to be deciphered with the following heuristics:\n",
    "\n",
    "- Text boxes containing left justified columns will tend to have nearly the same x0 coordinates\n",
    "- Text boxes containing right justified columns will tend to have nearly the same x1 coordinates\n",
    "- The codes for fund, account code, and object code are numeric and have fixed lengths\n",
    "- Extraneous information is often preceded or followed by a series of underscore and newline characters\n",
    "- Last name can be distinguished because is the only field that is all characters followed by a comma\n",
    "- Last name may be preceded by between one and three numerical fields:  fund, account, object.  If it is, the x0 value is shifted to the left.\n",
    "    - Three numerical fields precede the name:  assume they are fund, account, object\n",
    "    - Two numerical fields precede the name: assume they are account, object\n",
    "    - One numerical field precedes the name: assume it is object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    document = open(path, 'rb')                                     #read a pdf and create a document object\n",
    "    rsrcmgr = PDFResourceManager()                                  #create a resource manager\n",
    "    laparams = LAParams()                                           #set the parameters for analysis\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)          #create a PDF page aggregator object\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    pdf={}                                                          #dictionary to hold the results\n",
    "\n",
    "    pageno = -1                                                     #initialize page coounter to zero\n",
    "\n",
    "    for page in PDFPage.get_pages(document):                        #loop through the pdf page by page\n",
    "        pageno = pageno + 1                                         #increment the page number\n",
    "        pdf[pageno] = {}                                            #dictionary for this page\n",
    "        interpreter.process_page(page)                              # receive the LTPage object for the page.\n",
    "        layout = device.get_result()                                # create layout object\n",
    "        tbox_no=0                                                   # index for element number\n",
    "        for element in layout:\n",
    "            if (type(element).__name__=='LTTextBoxHorizontal'):     #loop through text boxes\n",
    "                tbox_no += 1                                        #increment text box number\n",
    "                pdf[pageno][tbox_no] = {}                           #dictionary for text boxes within page\n",
    "                x0 = round(element.x0,2)                            #x0 coordinate of textbox corner\n",
    "                x1 = round(element.x1,2)                            #x1 coordinate of textbox corner\n",
    "                y0 = round(element.y0,2)                            #y0 coordinate of textbox corner\n",
    "                y1 = round(element.y1,2)                            #y1 coordinate of textbox corner\n",
    "                txt = element.get_text().encode('ascii', 'ignore')  #text converted to ascii\n",
    "                pdf[pageno][tbox_no]['x0'] = x0                     #create x0 coordinate entry\n",
    "                pdf[pageno][tbox_no]['x1'] = x1                     #create x1 coordinate entry\n",
    "                pdf[pageno][tbox_no]['y0'] = y0                     #create y0 coordinate entry\n",
    "                pdf[pageno][tbox_no]['y1'] = y1                     #create y1 coordinate entry\n",
    "\n",
    "                pdf[pageno][tbox_no]['text'] = ''.join(chr(c) for c in txt) #convert bytes to string\n",
    "    return(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the commas from earnings and rate values\n",
    "\n",
    "def remove_commas(st):\n",
    "    newstr = st.replace(',','')                     #remove commas from string\n",
    "    return(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the headings fields \n",
    "\n",
    "def remove_headings(st):\n",
    "    lines = st.split('\\n')                         #split the string at newline characters '\\n'\n",
    "    for line in lines:                             #loop through the resulting lines\n",
    "        if (line.startswith('FUND ') |\\\n",
    "           (line.startswith('POSITION')) |\\\n",
    "           (line.startswith('RATE')) |\\\n",
    "           (line.startswith('ACCT-')) |\\\n",
    "           (line.startswith('CHECK')) |\\\n",
    "           (line.startswith('_'))):                #check for strings that appear only in headings\n",
    "            try:\n",
    "                newline_index = st.index('\\n')     #if present, remove this line from the text string\n",
    "                st = st[newline_index+1:]\n",
    "            except ValueError:\n",
    "                print('Value Error',st)            #recover from Value Error and print string\n",
    "        else:\n",
    "            return(st)                             #if no headings, just return\n",
    "    return('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_ = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2015 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_[2015] = read_pdf('../earnings/Munis_7-1-2014_to_6-30-2015.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2016 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_[2016] = read_pdf('../earnings/Munis_7-1-2015_to_6-30-2016.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2017 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p17 = read_pdf('../FY17 Gene_Redacted.pdf')\n",
    "earnings_[2017] = read_pdf('../earnings/Munis_7-1-2016_to_6-30-2017.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2018 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p18 = read_pdf('../FY18 Gene_Redacted.pdf')\n",
    "earnings_[2018] = read_pdf('../earnings/Munis_7-1-2017_to_6-30-2018.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2019 earnings report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_[2019] = read_pdf('../earnings/Munis_7-1-2018_to_7-1-2019.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the FY2020 through current report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_[2020] = read_pdf('../earnings/Munis_7-1-2019_to_current.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary with only those text boxes containing names\n",
    "\n",
    "Use the following algorithm to identify text boxes that contain names:\n",
    "\n",
    "- x0, horizontal coordinate of the upper left corner of the text box, is less than 162\n",
    "- the text string contains at least one comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(dct):\n",
    "\n",
    "    dnames = {}\n",
    "\n",
    "    fund = ''\n",
    "    acct = ''\n",
    "    obj  = ''\n",
    "    \n",
    "    for page in sorted(dct.keys()):                                #loop through text box dictionary by page # \n",
    "        if (page not in dnames.keys()):                            #page number is highest level key\n",
    "            dnames[page] = {}                                      #initialize entry for this page\n",
    "        for tb in sorted(dct[page].keys()):                        #loop through all text boxes on this page\n",
    "            if (dct[page][tb]['x0'] < 162.0):                      #those with names start to the left of x0=162\n",
    "                txt = str(dct[page][tb]['text'])                   #convert the 'text' element to a string\n",
    "                if (',' in txt):                                   #every name contains a comma\n",
    "                    txt = remove_headings(txt)\n",
    "                    lines = txt.split('\\n')                        #split text into lines\n",
    "                    words = lines[0].split()                       #split first line into words\n",
    "                    for word in words:                             #loop through and strip out fund, acct, obj\n",
    "                        if (word.isdigit()):\n",
    "                            if (len(word)==4):                     # 4 digits means fund\n",
    "                                fund = word\n",
    "                            if (len(word)==8):                     # 8 digits means acct-code\n",
    "                                acct = word\n",
    "                            if (len(word)==5):                     # 5 digits means obj\n",
    "                                obj = word\n",
    "                            txt = txt[len(word)+1:]                # remove fund/acct/obj from txt\n",
    "                    dnames[page][tb] = {}                          #initialize dictionary for this page\n",
    "                    dnames[page][tb]['x0'] = dct[page][tb]['x0']\n",
    "                    dnames[page][tb]['x1'] = dct[page][tb]['x1']\n",
    "                    dnames[page][tb]['y0'] = dct[page][tb]['y0']\n",
    "                    dnames[page][tb]['y1'] = dct[page][tb]['y1']\n",
    "                    dnames[page][tb]['fund'] = fund\n",
    "                    dnames[page][tb]['acct'] = acct\n",
    "                    dnames[page][tb]['obj'] = obj\n",
    "                    dnames[page][tb]['text'] = txt\n",
    "    return(dnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate text boxes that overlap on the vertical scale and contain names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_name_boxes(names):\n",
    "    newnames = {}\n",
    "    \n",
    "    for page in sorted(names.keys()):                                        #loop through pages of pdf\n",
    "        newnames[page] = {}                                                  #initialize new names dictionary\n",
    "        skip = make_array()                                                  #initialize list of boxes to skip\n",
    "    \n",
    "        for tb in sorted(names[page].keys()):                                #loop through text boxes on this page\n",
    "            for tb2 in sorted(names[page].keys()):                           #compare this one to the others\n",
    "                if ((tb2 > tb) & \\\n",
    "                    (names[page][tb]['y0'] <= names[page][tb2]['y1']) & \\\n",
    "                    (names[page][tb2]['y0'] <= names[page][tb]['y1'])):      \n",
    "                    d = {}                                                   #initialize replacement entry\n",
    "                    d['x0'] = names[page][tb]['x0']                          #keep x0    \n",
    "                    d['x1'] = names[page][tb2]['x1']                         #replace x1 with tb2 value\n",
    "                    d['y0'] = names[page][tb2]['y0']                         #replace y0 with tb2 value\n",
    "                    d['y1'] = names[page][tb]['y1']                          #keep y1 value\n",
    "                    d['text'] = names[page][tb]['text'] +\\\n",
    "                        names[page][tb2]['text']                             #contatenate text strings\n",
    "                    d['fund'] = names[page][tb]['fund']                      #copy fund, acct, and obj\n",
    "                    d['acct'] = names[page][tb]['acct']\n",
    "                    d['obj'] = names[page][tb]['obj']\n",
    "                    newnames[page][tb2] = d                                  #plug into dictionary\n",
    "                    skip = np.append(skip,tb)                                #add old boxes to skip list\n",
    "                    skip = np.append(skip,tb2)\n",
    "            if (tb not in skip):                                             #if no match, check skip list \n",
    "                newnames[page][tb] = names[page][tb]                         #just copy if not in skip list\n",
    "                    \n",
    "    return(newnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combdd(cn,pdf):\n",
    "    \n",
    "    dd = {}\n",
    "    \n",
    "    for page in sorted(cn.keys()):\n",
    "        if page not in dd.keys():\n",
    "            dd[page] = {}\n",
    "        for tb in sorted(cn[page].keys()):                               #loop through consolidated name textboxes\n",
    "            dd[page][tb] = cn[page][tb]\n",
    "            y0  = dd[page][tb]['y0']                                      #extract vertical coordinates\n",
    "            y1  = dd[page][tb]['y1']\n",
    "            txt = dd[page][tb]['text']                           #extract text\n",
    "            for tb2 in sorted(pdf[page].keys()):                            #loop through the other boxes in pdf\n",
    "                if (tb != tb2):                                             #ignore if same box as names\n",
    "                    tx0 = pdf[page][tb2]['x0']                              #get horizontal offset\n",
    "                    ty0 = pdf[page][tb2]['y0']                              #check whether the vertical \n",
    "                    ty1 = pdf[page][tb2]['y1']                              #range of this box overlaps that\n",
    "                    if ((y0 <= ty1) & (ty0 <= y1)):                         #of the name box\n",
    "                        txt = remove_headings(pdf[page][tb2]['text'])\n",
    "                        if ((312.0 < tx0) & (tx0 < 316.0)):                 #match to DATE/NUMBER\n",
    "                            dd[page][tb]['numbers1'] = txt\n",
    "                        if ((383.0 < tx0) & (tx0 < 395.0)):                 #match to NUMBER\n",
    "                            if 'numbers2' not in dd[page][tb].keys():\n",
    "                                dd[page][tb]['numbers2'] = txt\n",
    "                            else:\n",
    "                                dd[page][tb]['numbers2'] += txt\n",
    "                        if ((437.0 < tx0) & (tx0 < 440.0)):                 #match to POSITION\n",
    "                            dd[page][tb]['positions'] = txt\n",
    "                        if ((509.0 < tx0) & (tx0 < 533.0)):                 #match to RATE \n",
    "                            dd[page][tb]['rates'] = remove_commas(txt)\n",
    "                        if ((558.0 < tx0) & (tx0 < 630.0)):                 #match to ACCT-EARNINGS\n",
    "                            dd[page][tb]['earnings'] = remove_commas(txt)\n",
    "\n",
    "    return(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(nn):\n",
    "    \n",
    "    lld = {}\n",
    "    \n",
    "    for page in sorted(nn.keys()):\n",
    "        if page not in lld.keys():\n",
    "            lld[page] = {}\n",
    "        for tb in sorted(nn[page].keys()):\n",
    "            if tb not in lld[page].keys():\n",
    "                lld[page][tb]              = {}\n",
    "                lld[page][tb]['names']     = []\n",
    "                lld[page][tb]['checks']    = []\n",
    "                lld[page][tb]['dates']     = []\n",
    "                lld[page][tb]['rates']     = []\n",
    "                lld[page][tb]['earnings']  = []\n",
    "                lld[page][tb]['positions'] = []\n",
    "                lld[page][tb]['fund']      = ''\n",
    "                lld[page][tb]['acct']      = ''\n",
    "                lld[page][tb]['obj']       = ''\n",
    "            txt = nn[page][tb]['text']\n",
    "            words = txt.split('\\n')\n",
    "            for word in words:\n",
    "                if (len(word) > 1):\n",
    "                    lld[page][tb]['names'].append(word)\n",
    "            if 'numbers1' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['numbers1']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if word.isdigit():\n",
    "                        lld[page][tb]['checks'].append(word)\n",
    "                    elif '/' in word:\n",
    "                        lld[page][tb]['dates'].append(word)\n",
    "            if 'numbers2' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['numbers2']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if word.isdigit():\n",
    "                        lld[page][tb]['checks'].append(word)\n",
    "            if 'rates' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['rates']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if '.' in word:\n",
    "                        lld[page][tb]['rates'].append(float(word))\n",
    "            if 'positions' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['positions']\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if len(word)>1:\n",
    "                        lld[page][tb]['positions'].append(word)\n",
    "            if 'fund' in nn[page][tb].keys():\n",
    "                lld[page][tb]['fund'] = nn[page][tb]['fund']\n",
    "            if 'acct' in nn[page][tb].keys():\n",
    "                lld[page][tb]['acct'] = nn[page][tb]['acct']\n",
    "            if 'obj' in nn[page][tb].keys():\n",
    "                lld[page][tb]['obj'] = nn[page][tb]['obj']\n",
    "            if 'earnings' in nn[page][tb].keys():\n",
    "                txt = nn[page][tb]['earnings']\n",
    "                had_underscore = False\n",
    "                words = txt.split('\\n')\n",
    "                for word in words:\n",
    "                    if '.' in word:\n",
    "                        if not had_underscore: \n",
    "                            lld[page][tb]['earnings'].append(float(word))\n",
    "                            had_underscore = False\n",
    "                    elif '_' in word:\n",
    "                        had_underscore = True\n",
    "            if (len(lld[page][tb]['checks']) < len(lld[page][tb]['dates'])):\n",
    "                new_checks = []\n",
    "                check_index = 0\n",
    "                for i in np.arange(len(lld[page][tb]['earnings'])):\n",
    "                    if (lld[page][tb]['earnings'][i] > 0.0):\n",
    "                        new_checks.append(lld[page][tb]['checks'][check_index])\n",
    "                        check_index += 1\n",
    "                    else:\n",
    "                        new_checks.append('gen'+str(page) + '-' + str(tb) + '-' + str(i))\n",
    "                        print(\"inserting check number: \",page,tb,i)\n",
    "                lld[page][tb]['checks'] = new_checks\n",
    "    return(lld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the earnings reports and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "2016\n",
      "inserting check number:  85 7 10\n",
      "inserting check number:  85 7 13\n",
      "inserting check number:  85 7 14\n",
      "inserting check number:  157 6 48\n",
      "2017\n",
      "inserting check number:  262 6 39\n",
      "inserting check number:  262 6 40\n",
      "2018\n",
      "inserting check number:  43 6 30\n",
      "inserting check number:  51 6 49\n",
      "inserting check number:  94 7 19\n",
      "2019\n",
      "inserting check number:  259 6 9\n",
      "inserting check number:  259 6 28\n",
      "inserting check number:  259 6 39\n",
      "inserting check number:  259 6 40\n",
      "inserting check number:  259 6 43\n",
      "inserting check number:  259 6 45\n",
      "inserting check number:  273 23 1\n",
      "2020\n",
      "inserting check number:  133 7 0\n",
      "inserting check number:  133 7 3\n",
      "inserting check number:  133 7 6\n",
      "inserting check number:  136 11 0\n",
      "inserting check number:  136 11 3\n",
      "inserting check number:  140 7 0\n",
      "inserting check number:  140 7 3\n",
      "inserting check number:  143 9 0\n",
      "inserting check number:  143 9 3\n",
      "inserting check number:  143 9 5\n",
      "inserting check number:  144 10 0\n",
      "inserting check number:  148 7 0\n",
      "inserting check number:  148 7 3\n",
      "inserting check number:  148 7 5\n",
      "inserting check number:  151 11 0\n",
      "inserting check number:  152 10 0\n",
      "inserting check number:  155 7 0\n",
      "inserting check number:  155 7 3\n",
      "inserting check number:  158 6 29\n",
      "inserting check number:  159 11 5\n",
      "inserting check number:  160 7 0\n",
      "inserting check number:  177 7 4\n",
      "inserting check number:  183 15 0\n",
      "inserting check number:  249 7 0\n",
      "inserting check number:  249 7 3\n",
      "inserting check number:  257 12 2\n",
      "inserting check number:  257 12 5\n",
      "inserting check number:  257 12 8\n"
     ]
    }
   ],
   "source": [
    "def process_earnings(pdf):\n",
    "    nnd = get_names(pdf)\n",
    "    cnd = consolidate_name_boxes(nnd)\n",
    "    newnames = combdd(cnd,pdf)\n",
    "    lld = get_lines(newnames)\n",
    "    return(lld)\n",
    "\n",
    "ll={}\n",
    "\n",
    "for key in sorted(earnings_.keys()):\n",
    "    print(key)\n",
    "    ll[key] = process_earnings(earnings_[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check earnings against totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 20527796.79\n",
      "2016 21988876.13\n",
      "2017 22608024.34\n",
      "2018 22409915.41\n",
      "2019 23372079.04\n",
      "2020 19489886.57\n",
      "130396578.27999717\n"
     ]
    }
   ],
   "source": [
    "totearn = {}\n",
    "\n",
    "gtot = 0.0\n",
    "\n",
    "for year in ll.keys():\n",
    "    if year not in totearn.keys():\n",
    "        totearn[year] = 0.0\n",
    "    for page in ll[year].keys():\n",
    "        for tb in ll[year][page].keys():\n",
    "            for amt in ll[year][page][tb]['earnings']:\n",
    "                totearn[year] += amt\n",
    "    gtot += totearn[year]\n",
    "    print(year,round(totearn[year],2))\n",
    "\n",
    "#print(round(totearn[2015],2))       #FY2015 earnings report total is $20,527,796.79\n",
    "#print(round(totearn[2016],2))       #FY2016 earnings report total is $21,988,876.13\n",
    "#print(round(totearn[2017],2))       #FY2017 earnings report total is $22,608,024.34\n",
    "#print(round(totearn[2018],2))       #FY2018 earnings report total is $22,409,915.41\n",
    "#print(round(totearn[2019],2))       #FY2019 earnings report total is $23,372,079.04\n",
    "#print(round(totearn[2020],2))       #FY2020ytd earnings report total is $19,489,886.57\n",
    "print(gtot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write ll structure to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = date.today()\n",
    "fname = '../../finance_subcommittee/ll_' + str(current_date.month) + '_' + \\\n",
    "    str(current_date.day) + '_' + str(current_date.year) + '.pkl'\n",
    "with open(fname, 'wb') as handle:\n",
    "    pickle.dump(ll, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
